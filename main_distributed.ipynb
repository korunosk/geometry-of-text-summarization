{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.qhull import QhullError\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "from lexrank import degree_centrality_scores\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'TAC2008'\n",
    "TOPICS = ['D0841', 'D0804', 'D0802', 'D0809', 'D0819', 'D0825', 'D0828', 'D0826', 'D0843', 'D0829', 'D0813', 'D0807', 'D0812', 'D0820', 'D0835', 'D0823', 'D0847', 'D0848', 'D0810', 'D0822', 'D0845', 'D0844', 'D0839', 'D0814', 'D0824', 'D0821', 'D0827', 'D0846', 'D0818', 'D0834', 'D0805', 'D0817', 'D0831', 'D0815', 'D0836', 'D0806', 'D0808', 'D0837', 'D0803', 'D0830', 'D0838', 'D0840', 'D0842', 'D0832', 'D0816', 'D0801', 'D0833', 'D0811']\n",
    "DATA_DIR = f'/scratch/korunosk/data/{DATASET}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def load_and_extract(t):\n",
    "    with open(os.path.join(DATA_DIR, t + '_encoded.json'), mode='r') as fp:\n",
    "        topic = json.load(fp)\n",
    "    \n",
    "    documents = np.array(list(chain(*topic['documents'])))\n",
    "    annotations = topic['annotations']\n",
    "\n",
    "    summaries_tmp = list(map(itemgetter('text'), annotations))\n",
    "    indices_tmp = np.cumsum([0] + list(map(len, summaries_tmp)))\n",
    "    summaries = np.array(list(chain(*summaries_tmp)))\n",
    "    indices = np.array(list(zip(indices_tmp[:-1], indices_tmp[1:])))\n",
    "    \n",
    "    pyr_scores = np.array(list(map(itemgetter('pyr_score'), annotations)))\n",
    "    \n",
    "    return documents, summaries, indices, pyr_scores\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def experiment_average_pairwise_distance(data):\n",
    "    document_embs, summary_embs, indices, pyr_scores = data\n",
    "    \n",
    "    def average_pairwise_distance(summary_embs: np.array) -> float:\n",
    "        '''Calculates the average pairwise distance between summary embeddings'''\n",
    "        return np.mean(cdist(summary_embs, summary_embs, metric='euclidean'))\n",
    "    \n",
    "    metric = lambda i: average_pairwise_distance(summary_embs[i[0]:i[1]])\n",
    "    \n",
    "    return kendalltau(pyr_scores, np.array([metric(i) for i in indices]))[0]\n",
    "\n",
    "@ray.remote\n",
    "def experiment_semantic_volume(data):\n",
    "    document_embs, summary_embs, indices, pyr_scores = data\n",
    "    \n",
    "    embs = np.concatenate((document_embs, summary_embs))\n",
    "    t = document_embs.shape[0]\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pts = pca.fit_transform(embs)\n",
    "    document_pts, summary_pts = pts[:t], pts[t:]\n",
    "    \n",
    "    def semantic_volume(summary_embs: np.array) -> float:\n",
    "        '''Calculates the semantic volume of the summary embeddings'''\n",
    "        try:\n",
    "            return ConvexHull(summary_embs).volume\n",
    "        except QhullError as e:\n",
    "            return 0\n",
    "    \n",
    "    metric = lambda i: semantic_volume(summary_pts[i[0]:i[1]])\n",
    "    \n",
    "    return kendalltau(pyr_scores, np.array([metric(i) for i in indices]))[0]\n",
    "\n",
    "@ray.remote\n",
    "def experiment_semantic_spread(data):\n",
    "    document_embs, summary_embs, indices, pyr_scores = data\n",
    "\n",
    "    def semantic_spread(summary_embs: np.array) -> float:\n",
    "        '''Calculates the semantic spread of the summary embeddings'''\n",
    "        return np.linalg.det(summary_embs @ summary_embs.T)\n",
    "\n",
    "    metric = lambda i: semantic_spread(summary_embs[i[0]:i[1]])\n",
    "    \n",
    "    return kendalltau(pyr_scores, np.array([metric(i) for i in indices]))[0]\n",
    "\n",
    "\n",
    "def execute_experiment(experiment):\n",
    "    data   = [ load_and_extract.remote(t) for t in TOPICS ]\n",
    "    scores = [ experiment.remote(d) for d in data ]\n",
    "\n",
    "    return np.array(ray.get(scores))\n",
    "\n",
    "scores = execute_experiment(experiment_semantic_spread)\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(TOPICS))\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.bar(x, scores, width=0.2, label='Semantic Spread')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(TOPICS, rotation=90)\n",
    "ax.set_title(DATASET)\n",
    "ax.set_xlabel('topic')\n",
    "ax.set_ylabel('kendalltau')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
